# Don't try to use this in the crawler.
# It will break things.

# Number of threads to crawl with.
THREAD_COUNT = <Int>

# Whether to overwrite the current save files or not.
OVERWRITE = <True/False>

# Whether to stop the crawler after hitting an unknown error.
RAISE_ERRORS = <True/False>

# Whether to save pages after crawling.
SAVE_PAGES = <True/False>

# Whether to scrape and process words.
SAVE_WORDS = <True/False>

# Whether to zip saved pages when autosaving.
ZIP_FILES = <True/False>

# Whether to get documents larger than 500 MB
OVERRIDE_SIZE = <True/False>

# Whether to restrict crawling to a single domain or not.
RESTRICT = <True/False>

# The domain within which to restrict crawling.
DOMAIN = ''

# Whether to respect sites' robots.txt or not
RESPECT_ROBOTS = <True/False>

# Location of the TODO save file.
TODO_FILE = ''

# Location of the done save file.
DONE_FILE = ''

# Location of the word save file.
WORD_FILE = ''

# Number of queried links after which to autosave.
SAVE_COUNT = <Int>

# Which browser the crawler should headers for. (spidy, Chrome, IE, Edge)
HEADER = HEADERS['<Header>']
# Or if you want to use custom headers:
HEADER = {'<Header Name>': '<Value>', '<Header2>': '<Value2>'}

# Amount of errors allowed to happen before automatic shutdown.
MAX_NEW_ERRORS = <Int>
MAX_KNOWN_ERRORS = <Int>
MAX_HTTP_ERRORS = <Int>
MAX_NEW_MIMES = <Int>

# Pages to start crawling on in case TODO is empty at start.
START = ['', '']
